{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from scipy.io import wavfile\n",
    "from sklearn import preprocessing\n",
    "import librosa\n",
    "from scipy.stats import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('voice_gender_model.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(y,sr):\n",
    "    \n",
    "    meanfreq = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr))\n",
    "    sd = np.std(librosa.feature.mfcc(y=y, sr=sr))\n",
    "    median = np.median(librosa.feature.mfcc(y=y, sr=sr))\n",
    "    q25 = np.percentile(librosa.feature.mfcc(y=y, sr=sr), 25)\n",
    "    q75 = np.percentile(librosa.feature.mfcc(y=y, sr=sr), 75)\n",
    "    iqr = q75 - q25\n",
    "    skew = np.mean(librosa.feature.spectral_bandwidth(y=y, sr=sr, p=3))  \n",
    "    kurt = np.mean(librosa.feature.spectral_bandwidth(y=y, sr=sr, p=4))  \n",
    "    sp_ent = np.mean(librosa.feature.spectral_flatness(y=y))\n",
    "    sfm = np.mean(librosa.feature.spectral_flatness(y=y))\n",
    "    hist, edges = np.histogram(y, bins=np.arange(256))\n",
    "    mode_value = edges[np.argmax(hist)]\n",
    "    centroid = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr))\n",
    "    meanfun = np.mean(librosa.feature.mfcc(y=y, sr=sr))\n",
    "    maxfun = np.max(librosa.feature.mfcc(y=y, sr=sr))\n",
    "    meandom = np.mean(librosa.feature.mfcc(y=y, sr=sr))\n",
    "    minfun = np.min(librosa.feature.mfcc(y=y, sr=sr))\n",
    "    mindom = np.min(librosa.feature.mfcc(y=y, sr=sr))\n",
    "    maxdom = np.max(librosa.feature.mfcc(y=y, sr=sr))\n",
    "    dfrange = maxdom - mindom\n",
    "    modindx = np.mean(librosa.feature.mfcc(y=y, sr=sr))\n",
    "    \n",
    "    features=np.array([meanfreq, sd, median, q25, q75, iqr, skew, kurt, sp_ent, sfm, mode_value, centroid, meanfun, minfun, maxfun, meandom, mindom, maxdom, dfrange, modindx])\n",
    "    return features.reshape(1,-1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(audio_file):\n",
    "    # Load audio file and extract features (adjust based on your actual feature extraction method)\n",
    "    y, sr = librosa.load(audio_file)\n",
    "    # Extract features (adjust based on your actual feature extraction method)\n",
    "    features = extract_features(y, sr)\n",
    "    # Normalize the features using the same scaler used during training\n",
    "    features_normalized = preprocessing.MinMaxScaler().fit_transform(features)\n",
    "    return features_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_gender(audio_file):\n",
    "    # Preprocess the audio file\n",
    "    processed_audio = preprocess_audio(audio_file)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = model.predict(processed_audio)\n",
    "    print(predictions[0][0])\n",
    "    # Assuming binary classification (male/female)\n",
    "    gender = \"Male\" if predictions[0][0] > 0.5 else \"Female\"\n",
    "\n",
    "    return gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "import wave\n",
    "\n",
    "def record_and_save_wav(filename, duration=5):\n",
    "    # Initialize the recognizer\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Recording... Speak now!\")\n",
    "        audio_data = recognizer.listen(source, timeout=duration)\n",
    "        print(\"Recording complete.\")\n",
    "\n",
    "    # Save audio data to a WAV file\n",
    "    with wave.open(filename, 'wb') as wf:\n",
    "        wf.setnchannels(1)  # Mono audio\n",
    "        wf.setsampwidth(2)  # 16-bit audio\n",
    "        wf.setframerate(44100)  # Sample rate\n",
    "        wf.writeframes(audio_data.frame_data)\n",
    "\n",
    "    print(f\"Audio saved as {filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 68ms/step\n",
      "0.9160546\n",
      "Predicted Gender: Male\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Replace 'your_audio_file.wav' with the path to the user's voice file\n",
    "    output_filename = 'user_input.wav'\n",
    "    #record_and_save_wav(output_filename)\n",
    "    user_audio_file = 'amy.wav'\n",
    "    \n",
    "    predicted_gender = predict_gender(user_audio_file)\n",
    "\n",
    "    print(f\"Predicted Gender: {predicted_gender}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
